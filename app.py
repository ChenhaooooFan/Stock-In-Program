import streamlit as st
import pandas as pd
import re

# ============== é¡µé¢è®¾ç½® ==============
st.set_page_config(page_title="NailVesta å…¥åº“åŒ¹é…å·¥å…·ï¼ˆæ”¯æŒ PDFï¼‰", layout="wide")
st.title("ğŸ’… NailVesta å…¥åº“åŒ¹é…å·¥å…·ï¼ˆä»…è¾“å‡ºå…¥åº“æ•°é‡ï½œæ”¯æŒ CSV/XLSX/PDFï¼‰")

# ============== å·¥å…·å‡½æ•° ==============
@st.cache_data(show_spinner=False)
def read_any_table(file):
    """è¯»å– CSV/XLSX ä¸º DataFrameï¼›PDF ä½¿ç”¨ pdfplumber è§£æä¸º DataFrameï¼ˆè‡ªåŠ¨æ‰¾è¡¨ã€æ¸…æ´—åˆ—ï¼‰"""
    name = file.name.lower()
    if name.endswith(".csv"):
        # å…¼å®¹å¸¦ BOM
        try:
            return pd.read_csv(file, encoding="utf-8-sig")
        except Exception:
            file.seek(0)
            return pd.read_csv(file)
    elif name.endswith(".xlsx"):
        return pd.read_excel(file)
    elif name.endswith(".pdf"):
        # å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…é PDF ä¹Ÿè¦æ±‚å®‰è£… pdfplumber
        import pdfplumber
        return read_pdf_to_df(file)
    else:
        raise ValueError("ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œè¯·ä¸Šä¼  CSV / XLSX / PDF")

def _normalize_header(s):
    if s is None: 
        return ""
    s = str(s).strip()
    s = s.replace("\n", "").replace(" ", "")
    s = s.replace("ï¼ˆ", "(").replace("ï¼‰", ")")
    return s

def read_pdf_to_df(file_obj) -> pd.DataFrame:
    """ä½¿ç”¨ pdfplumber è§£æ PDF è¡¨æ ¼ï¼Œè¾“å‡ºè‡³å°‘åŒ…å« SKU ç¼–ç ã€Sæ•°é‡ã€Mæ•°é‡ã€Læ•°é‡ å››åˆ—ï¼ˆè‹¥èƒ½è¯†åˆ«ï¼‰ã€‚"""
    import pdfplumber
    rows = []
    with pdfplumber.open(file_obj) as pdf:
        for page in pdf.pages:
            page_tables = page.extract_tables() or []
            for tbl in page_tables:
                if not tbl or len(tbl) < 2:
                    continue
                header = [_normalize_header(x) for x in tbl[0]]
                width = len(header)
                if width < 2:
                    continue
                for r in tbl[1:]:
                    rr = list(r) + [""] * max(0, width - len(r))
                    rr = rr[:width]
                    rows.append(dict(zip(header, rr)))
    if not rows:
        raise ValueError("æœªåœ¨ PDF ä¸­è¯†åˆ«åˆ°å¯ç”¨è¡¨æ ¼ï¼Œè¯·ç¡®è®¤ PDF ä¸ºå¯å¤åˆ¶è¡¨æ ¼ï¼ˆéçº¯å›¾ç‰‡ï¼‰ã€‚")

    df = pd.DataFrame(rows)

    # æ ‡å‡†åŒ–åˆ—å
    norm_map = {c: _normalize_header(c).lower() for c in df.columns}
    df.columns = [norm_map[c] for c in df.columns]

    # è‡ªåŠ¨è¯†åˆ« 4 åˆ—
    sku_col, s_col, m_col, l_col = auto_pick_entry_columns(df)
    if not all([sku_col, s_col, m_col, l_col]):
        raise ValueError(f"PDF è§£æå®Œæˆï¼Œä½†æœªæ‰¾åˆ°å®Œæ•´åˆ—ï¼š'SKU ç¼–ç 'ã€'Sæ•°é‡'ã€'Mæ•°é‡'ã€'Læ•°é‡'ã€‚å½“å‰åˆ—ï¼š{list(df.columns)[:12]}...")

    out = df[[sku_col, s_col, m_col, l_col]].copy()
    out.columns = ["SKU ç¼–ç ", "Sæ•°é‡", "Mæ•°é‡", "Læ•°é‡"]

    # æ¸…æ´— SKU åŸºç ï¼ˆä¾‹å¦‚å»æ‰å¤‡æ³¨â€œè¡¥å¯„Mâ€ç­‰ï¼Œåªä¿ç•™å½¢å¦‚ NPX014ï¼‰
    def clean_base(s):
        s = str(s).strip()
        s = re.sub(r"\s+", "", s)
        s = s.replace("ï¼ˆ", "(").replace("ï¼‰", ")")
        s = re.sub(r"(è¡¥å¯„|åŠ å•|é‡å‘|å¤‡æ³¨).*", "", s)
        m = re.search(r"[A-Z]{2,4}\d{3}", s)
        return m.group(0) if m else s

    out["SKU ç¼–ç "] = out["SKU ç¼–ç "].apply(clean_base)

    for c in ["Sæ•°é‡", "Mæ•°é‡", "Læ•°é‡"]:
        out[c] = pd.to_numeric(out[c], errors="coerce").fillna(0).round().clip(lower=0).astype(int)

    return out

def auto_pick_entry_columns(df: pd.DataFrame):
    """ä» DataFrameï¼ˆåˆ—åå·²æ ‡å‡†åŒ–ä¸ºå°å†™ã€å»ç©ºæ ¼ï¼‰è‡ªåŠ¨æŒ‘é€‰ï¼šSKU/S/M/L åˆ—"""
    cols = list(df.columns)

    def pick_first(keys):
        for k in keys:
            if k in cols:
                return k
        # æ¨¡ç³ŠåŒ…å«
        for c in cols:
            for k in keys:
                if k in c:
                    return c
        return None

    # å¯èƒ½çš„åˆ—åï¼ˆå·²æ ‡å‡†åŒ–ï¼‰
    sku_keys = ["skuç¼–ç ", "sku", "skucode", "skuç¼–å·", "skuç·¨ç¢¼", "skuç¼–ç (å¿…å¡«)", "skuç¼–ç ï¼ˆå¿…å¡«ï¼‰", "å°ç›’å­", "ä¸‹å•ç¼–å·"]
    s_keys   = ["sæ•°é‡", "sqty", "sä»¶æ•°", "sç æ•°é‡", "s"]
    m_keys   = ["mæ•°é‡", "mqty", "mä»¶æ•°", "mç æ•°é‡", "m"]
    l_keys   = ["læ•°é‡", "lqty", "lä»¶æ•°", "lç æ•°é‡", "l"]

    sku_col = pick_first(sku_keys)
    s_col   = pick_first(s_keys)
    m_col   = pick_first(m_keys)
    l_col   = pick_first(l_keys)
    return sku_col, s_col, m_col, l_col

def build_entry_map(entry_df: pd.DataFrame, sku_col="SKU ç¼–ç ", s_col="Sæ•°é‡", m_col="Mæ•°é‡", l_col="Læ•°é‡"):
    """æ„å»º full_sku -> æ•°é‡ çš„æ˜ å°„ï¼ˆç´¯è®¡åŒä¸€ SKU å¤šè¡Œï¼‰"""
    # åˆ—åå®¹é”™ï¼šå°è¯•æ ‡å‡†åŒ–ä¸ºç›®æ ‡å‘½å
    cols_norm = {c: _normalize_header(c) for c in entry_df.columns}
    inv = {v: k for k, v in cols_norm.items()}
    sku_col = inv.get(_normalize_header(sku_col), sku_col)
    s_col   = inv.get(_normalize_header(s_col),   s_col)
    m_col   = inv.get(_normalize_header(m_col),   m_col)
    l_col   = inv.get(_normalize_header(l_col),   l_col)

    # è‹¥åˆ—åä¸åŒ¹é…ï¼Œå†è‡ªåŠ¨æŒ‘é€‰ä¸€æ¬¡
    needed = [sku_col, s_col, m_col, l_col]
    if any(col not in entry_df.columns for col in needed):
        # å°†åˆ—åæ ‡å‡†åŒ–åå†è‡ªåŠ¨æŒ‘é€‰
        df2 = entry_df.copy()
        df2.columns = [ _normalize_header(c).lower() for c in df2.columns ]
        sku_pick, s_pick, m_pick, l_pick = auto_pick_entry_columns(df2)
        if not all([sku_pick, s_pick, m_pick, l_pick]):
            raise ValueError(f"å…¥åº“è¡¨ç¼ºå°‘å¿…è¦åˆ—ï¼š{needed}ï¼Œä¸”è‡ªåŠ¨è¯†åˆ«å¤±è´¥ã€‚å½“å‰åˆ—ï¼š{list(entry_df.columns)[:12]}...")
        entry_df = df2[[sku_pick, s_pick, m_pick, l_pick]].copy()
        entry_df.columns = ["SKU ç¼–ç ", "Sæ•°é‡", "Mæ•°é‡", "Læ•°é‡"]
        sku_col, s_col, m_col, l_col = "SKU ç¼–ç ", "Sæ•°é‡", "Mæ•°é‡", "Læ•°é‡"

    # è§„èŒƒåŒ–æ•°é‡
    for c in [s_col, m_col, l_col]:
        entry_df[c] = pd.to_numeric(entry_df[c], errors="coerce").fillna(0).round().clip(lower=0).astype(int)

    # æ¸…æ´— SKU åŸºç 
    def clean_base(s):
        s = str(s).strip()
        s = re.sub(r"\s+", "", s)
        s = s.replace("ï¼ˆ", "(").replace("ï¼‰", ")")
        s = re.sub(r"(è¡¥å¯„|åŠ å•|é‡å‘|å¤‡æ³¨).*", "", s)
        m = re.search(r"[A-Z]{2,4}\d{3}", s)
        return m.group(0) if m else s

    entry_df[sku_col] = entry_df[sku_col].apply(clean_base)

    entry_map = {}
    for _, row in entry_df.iterrows():
        base = str(row[sku_col]).strip()
        for size, col in zip(["S", "M", "L"], [s_col, m_col, l_col]):
            qty = int(row[col])
            if qty > 0:
                full = f"{base}-{size}"
                entry_map[full] = entry_map.get(full, 0) + qty
    return entry_map, entry_df

# ============== ä¸Šä¼ æ–‡ä»¶åŒº ==============
stock_file = st.file_uploader("ğŸ“¦ ä¸Šä¼ åº“å­˜è¡¨ï¼ˆå¿…é¡»åŒ…å«ã€SKUç¼–ç ã€åˆ—ï¼Œå¦‚ NPX014-Sï¼‰", type=["csv", "xlsx"])
entry_file = st.file_uploader("ğŸ“ ä¸Šä¼ å…¥åº“è¡¨ï¼ˆæ”¯æŒ CSV / XLSX / PDFï¼Œéœ€å«ã€SKU ç¼–ç ã€ã€Sæ•°é‡ã€ã€Mæ•°é‡ã€ã€Læ•°é‡ã€ï¼‰", type=["csv", "xlsx", "pdf"])

# ============== ä¸»æµç¨‹ ==============
if stock_file and entry_file:
    try:
        stock_df = read_any_table(stock_file)
        entry_df_raw = read_any_table(entry_file)

        # åº“å­˜è¡¨æ ¡éªŒ
        if "SKUç¼–ç " not in stock_df.columns:
            st.error("âŒ åº“å­˜è¡¨ç¼ºå°‘ã€SKUç¼–ç ã€åˆ—ã€‚è¯·ç¡®ä¿è¡¨å¤´ä¸ºâ€œSKUç¼–ç â€ï¼ˆç¤ºä¾‹ï¼šNPX014-Sï¼‰ã€‚")
            st.stop()

        stock_skus = (
            stock_df["SKUç¼–ç "].dropna().astype(str).str.strip().tolist()
        )
        if len(stock_skus) == 0:
            st.error("âŒ åº“å­˜è¡¨ã€SKUç¼–ç ã€ä¸ºç©ºã€‚")
            st.stop()

        st.subheader("ğŸ§¾ å…¥åº“è¡¨é¢„è§ˆï¼ˆå‰ 20 è¡Œï¼‰")
        st.dataframe(entry_df_raw.head(20), use_container_width=True)

        # æ„å»ºå…¥åº“æ˜ å°„ï¼ˆè‡ªåŠ¨è¯†åˆ«åˆ—åï¼Œç´¯è®¡åŒ SKUï¼‰
        entry_map, entry_df_clean = build_entry_map(entry_df_raw)

        # ç”ŸæˆåŒ¹é…ç»“æœï¼Œä¸åº“å­˜è¡¨é¡ºåºé€è¡Œå¯¹é½
        results, unmatched, total_in = [], [], 0
        for sku in stock_skus:
            qty = entry_map.get(sku, "")
            results.append((sku, qty))
            if qty == "":
                unmatched.append(sku)
            else:
                total_in += qty

        result_df = pd.DataFrame(results, columns=["SKUç¼–ç ", "å…¥åº“æ•°é‡"])

        st.success(f"âœ… åŒ¹é…å®Œæˆï¼åŒ¹é…åˆ° {result_df['å…¥åº“æ•°é‡'].replace('', 0).astype(int).gt(0).sum()} ä¸ª SKUï¼Œç´¯è®¡ {total_in} ä»¶ã€‚")
        st.dataframe(result_df, use_container_width=True)

        st.markdown("### ğŸ“‹ å¯å¤åˆ¶å…¥åº“æ•°é‡åˆ—ï¼ˆä¸åº“å­˜è¡¨é¡ºåºä¸€ä¸€å¯¹åº”ï¼‰")
        copy_col = "\n".join([str(x) for x in result_df["å…¥åº“æ•°é‡"].tolist()])
        st.code(copy_col, language="text")

        st.download_button(
            "ğŸ“¥ ä¸‹è½½å…¥åº“æ•°é‡ CSV",
            result_df.to_csv(index=False).encode("utf-8-sig"),
            "NailVesta_å…¥åº“æ•°é‡.csv",
            "text/csv"
        )

        # è¯Šæ–­ä¿¡æ¯
        if unmatched:
            st.warning("âš ï¸ æœªåŒ¹é…åˆ°å…¥åº“æ•°é‡çš„åº“å­˜ SKUï¼ˆå‰ 10 æ¡ï¼‰ï¼š\n" + "\n".join(unmatched[:10]))

        with st.expander("ğŸ” è°ƒè¯•ä¿¡æ¯ï¼ˆè§£æåçš„å…¥åº“è¡¨å‰ 20 è¡Œ & åˆ—åï¼‰"):
            st.write("å…¥åº“è¡¨åˆ—åï¼š", list(entry_df_clean.columns))
            st.dataframe(entry_df_clean.head(20), use_container_width=True)

    except Exception as e:
        st.error(f"âŒ å‡ºé”™ï¼š{e}")

else:
    st.info("ğŸ‘† è¯·å…ˆä¸Šä¼ ã€åº“å­˜è¡¨ã€ä¸ã€å…¥åº“è¡¨ã€ï¼ˆCSV/XLSX/PDFï¼‰ã€‚")
